{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "import time\n",
    "\n",
    "\n",
    "from itertools import izip,tee \n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def augment_headlines(df,w2v,output_filename,nrows=None):\n",
    "    \"\"\"Takes a dataframe (train or test) and returns a new dataframe with new rows of sentences that have their words switched out for synonyms\n",
    "        Only does this for the first nrows, defaults to all\"\"\"\n",
    "    if nrows == None:\n",
    "        nrows = len(df)\n",
    "\n",
    "    ave_dur = []\n",
    "    #header= \"Index,Headline,Stance ID,Body ID,Stance,articleBody,New Body ID\"\n",
    "    header= \"Index|Headline|Stance ID|Body ID|Stance|articleBody|New Body ID\"\n",
    "\n",
    "    Indices = df.index\n",
    "    Headlines = df.Headline\n",
    "    StanceIDs = df.loc[:,\"Stance ID\"]\n",
    "    BodyIDs = df.loc[:,\"Body ID\"]\n",
    "    Stances = df.loc[:,\"Stance\"]\n",
    "    Bodies = df.loc[:,\"articleBody\"]\n",
    "    NewBodyIDs = df.loc[:,\"New Body ID\"]\n",
    "    \n",
    "    with open(output_filename,'wb') as write_file:\n",
    "        write_file.write(header + \"\\n\")\n",
    "        for i in xrange(nrows):\n",
    "            if i % 100 == 0:\n",
    "                print i\n",
    "            index = str(Indices[i])\n",
    "            headline = Headlines.iloc[i]\n",
    "            stanceID = StanceIDs.iloc[i]\n",
    "            bodyid = str(BodyIDs.iloc[i])\n",
    "            stance = Stances.iloc[i]\n",
    "            body = Bodies.iloc[i].replace('\\n','').replace(\"|\",'')\n",
    "            newbodyid = str(NewBodyIDs.iloc[i])\n",
    "\n",
    "            start_time = time.time()\n",
    "            new_sentences = generate_similar_sentences(headline,w2v)\n",
    "\n",
    "            for counter,new_sent in enumerate(new_sentences):\n",
    "                amended_stance_id = str(stanceID) + \"_\" + str(counter)\n",
    "                row = \"|\".join([str(index),str(new_sent),amended_stance_id,str(bodyid),str(stance),str(body),str(newbodyid)])\n",
    "                write_file.write(row + \"\\n\")\n",
    "\n",
    "            dur = time.time() - start_time\n",
    "            ave_dur.append(dur)\n",
    "            \n",
    "    print \"generated dur\" + str(sum(ave_dur)/len(ave_dur))\n",
    "    \n",
    "    return \n",
    "\n",
    "def generate_similar_sentences(sentence,w2v,percentage_to_replace=1,max_syn=10,num_outputs=50):\n",
    "    \"\"\"Takes a sentence, switches out non compond words, returning a list of similar sentences\n",
    "        it will ignore compound words and stop words if picked\n",
    "        max_syn is the max number of synonyms to look at for a given word \n",
    "        Num_outputs is number of sentences to return. \"\"\"\n",
    "\n",
    "    list_of_sentences = []\n",
    "\n",
    "    words = pairwise_tokenize(sentence,w2v,remove_stopwords=False) #This has combined any compound words found in word2vec\n",
    "\n",
    "    #if word contains underscore don't sub in synonyms\n",
    "    dont_sub_idx = []\n",
    "    compound_word_idx = []\n",
    "    deleted_idx = []\n",
    "    for idx,word in enumerate(words):\n",
    "        if \"_\" in word or word in stopwords.words('english'):\n",
    "            dont_sub_idx.append(idx)\n",
    "        if \"_\" in word:\n",
    "            compound_word_idx.append(idx)\n",
    "            deleted_idx.append(idx+1)\n",
    "        if not word:\n",
    "            dont_sub_idx.append(idx)\n",
    "\n",
    "    pattern = re.compile('[\\W_]+') \n",
    "    sentence = pattern.sub(\" \",sentence).lower().strip()\n",
    "    tagged = pos_tag(sentence.split(\" \")) #Pos_tag needs to use the original sentence to tag parts of speech, we will now delete indices that correspond to words that no longer exist b/c of compound\n",
    "    \n",
    "    for idx in reversed(compound_word_idx):\n",
    "        tagged.pop(idx+1)\n",
    "        \n",
    "    for tag in tagged:\n",
    "        if tag[1] == 'NNP':\n",
    "            dont_sub_idx.append(idx)\n",
    "            \n",
    "    for i in xrange(num_outputs):\n",
    "        new_words = words\n",
    "        mask = np.random.random_sample(len(words))\n",
    "        for j in xrange(len(words)):\n",
    "            if mask[j] < .5 and j not in dont_sub_idx:\n",
    "                pos = wordnet_pos_code(tagged[j][1])\n",
    "                synonyms = get_synonyms(words[j],w2v,pos,max=max_syn)\n",
    "                if len(synonyms) != 0:\n",
    "                    new_words[j] = synonyms[np.random.randint(0,min(max_syn,len(synonyms)))]\n",
    "        list_of_sentences.append(\" \".join(new_words))\n",
    "\n",
    "    list_of_sentences = set(list_of_sentences)\n",
    "    return list(list_of_sentences)\n",
    "\n",
    "def pairwise_tokenize(sentence,w2v,remove_stopwords=True):\n",
    "    \"\"\" Returns list of valid words + compound words\n",
    "        Naively looks at each pair of words in the sentence and checks if it is in word2vec\n",
    "        If so, it'll merge it. Then it'll remove stopwords. For the remaining words, it'll add the single word representation\"\"\"\n",
    "\n",
    "    ignore_words = stopwords.words('english')\n",
    "\n",
    "    #Remove non-alphanumeric\n",
    "    pattern = re.compile('[\\W_]+') \n",
    "    sentence = pattern.sub(\" \",sentence)  \n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(\" \")\n",
    "\n",
    "    compound_word_idx = []\n",
    "    a_idx = 0\n",
    "    for a,b in pairwise(words):\n",
    "        combined = a +\"_\" + b\n",
    "        try:\n",
    "            w2v[combined]\n",
    "            compound_word_idx.append(a_idx) #append the index of the 1st compound word\n",
    "            a_idx += 1\n",
    "        except KeyError:\n",
    "            a_idx += 1\n",
    "\n",
    "    for idx in compound_word_idx:\n",
    "        words[idx] = words[idx] + \"_\" + words[idx + 1] #compound_word_idx stores index of 1st word, so combine with the next word\n",
    "\n",
    "    #This cannot be combined into another loop to maintain where indices point\n",
    "    for idx in reversed(compound_word_idx):\n",
    "        words.pop(idx+1)\n",
    "\n",
    "    if remove_stopwords == True:\n",
    "        filtered = []\n",
    "        for word in words:\n",
    "            word = word.decode(\"utf-8\")\n",
    "            if word not in ignore_words:\n",
    "                filtered.append(word)\n",
    "\n",
    "        words = filtered\n",
    "\n",
    "    return words\n",
    "\n",
    "def get_synonyms(word,w2v,pos,max=20):\n",
    "    \"\"\"returns list of synonyms\"\"\"\n",
    "    synonyms = []\n",
    "    count = 0\n",
    "    synsets = wordnet.synsets(word,pos=pos)\n",
    "    for synset in synsets:\n",
    "        candidate_names = []\n",
    "        for lemma in synset.lemma_names():\n",
    "            candidate_names.append(lemma)\n",
    "        for hypo in synset.hyponyms():\n",
    "            candidate_names.append(hypo)\n",
    "        for hyper in synset.hypernyms():\n",
    "            candidate_names.append(hyper)\n",
    "\n",
    "        for lemma in candidate_names:\n",
    "            if count >= max:\n",
    "                break\n",
    "            # print pos,word,lemma\n",
    "            try:\n",
    "                similarity = w2v.n_similarity([word.lower()],[lemma.lower() ])\n",
    "                if similarity > .34 and lemma not in synonyms:\n",
    "                    synonyms.append(lemma)\n",
    "\n",
    "                    count += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return izip(a, b)\n",
    "\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/training-all.csv\")\n",
    "w2v_file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_url = 'https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
    "\n",
    "w2v = utils.load_w2v(w2v_file,w2v_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n"
     ]
    }
   ],
   "source": [
    "augmented_temp = augment_headlines(data,w2v,\"temp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "import time\n",
    "new_data = pd.read_csv(\"temp.txt\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Original data was 12300-12400"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aml]",
   "language": "python",
   "name": "conda-env-aml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
